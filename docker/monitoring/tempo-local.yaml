server:
  http_listen_port: 3200

distributor:
  search_tags_deny_list:
    - "instance"
    - "version"
  receivers:                           # this configuration will listen on all ports and protocols that tempo is capable of.
    jaeger:                            # the receives all come from the OpenTelemetry collector.  more configuration information can
      protocols:                       # be found there: https://github.com/open-telemetry/opentelemetry-collector/tree/main/receiver
        thrift_http:                   #
        grpc:                          # for a production deployment you should only enable the receivers you need!
        thrift_binary:
        thrift_compact:
    zipkin:
    otlp:
      protocols:
        http:
        grpc:
    opencensus:

ingester:
  # Lifecycler is responsible for managing the lifecycle of entries in the ring.
  # For a complete list of config options check the lifecycler section under the ingester config at the following link -
  # https://cortexmetrics.io/docs/configuration/configuration-file/#ingester_config
  lifecycler:
    ring:
      # number of replicas of each span to make while pushing to the backend
      replication_factor: 3
  trace_idle_period: 10s               # the length of time after a trace has not received spans to consider it complete and flush it
  max_block_bytes: 1_000_000           # cut the head block when it hits this size or ...
  max_block_duration: 5m               #   this much time passes

compactor:
  ring:
    kvstore:
  compaction:
    compaction_window: 1h              # blocks in this time window will be compacted together
    max_block_bytes: 100_000_000       # maximum size of compacted blocks
    block_retention: 1h
    compacted_block_retention: 10m

metrics_generator:
  ring:
    kvstore:
  # Processor-specific configuration
  processor:
    service_graphs:
    span_metrics:
  registry:
    external_labels:
      source: tempo
      cluster: docker-compose
  storage:
    # Path to store the WAL. Each tenant will be stored in its own subdirectory.
    path: /tmp/tempo/generator/wal
    # Configuration for the Prometheus Agent WAL
    wal:
    # A list of remote write endpoints.
    # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true

storage:
  trace:
    backend: local                     # backend configuration to use
    block:
      bloom_filter_false_positive: .05 # bloom filter false positive rate.  lower values create larger filters but fewer false positives
      v2_index_downsample_bytes: 1000  # number of bytes per index record
      v2_encoding: zstd                # block encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2
    wal:
      path: /tmp/tempo/wal             # where to store the the wal locally
      v2_encoding: snappy              # wal encoding/compression.  options: none, gzip, lz4-64k, lz4-256k, lz4-1M, lz4, snappy, zstd, s2
    local:
      path: /tmp/tempo/blocks
    pool:
      max_workers: 100                 # worker pool determines the number of parallel requests to the object store backend
      queue_depth: 10000

overrides:
  per_tenant_override_config: /etc/overrides.yaml
  metrics_generator_processors: [service-graphs, span-metrics]
  max_bytes_per_trace: 9900000
